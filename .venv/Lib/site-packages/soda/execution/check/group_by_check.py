from __future__ import annotations

import copy

from soda.cloud.historic_descriptor import (
    HistoricChangeOverTimeDescriptor,
    HistoricCheckResultsDescriptor,
    HistoricMeasurementsDescriptor,
)
from soda.execution.check.anomaly_metric_check import (
    HISTORIC_MEASUREMENTS_LIMIT,
    KEY_HISTORIC_CHECK_RESULTS,
    KEY_HISTORIC_MEASUREMENTS,
    AnomalyMetricCheck,
)
from soda.execution.check.change_over_time_metric_check import (
    KEY_HISTORIC_METRIC_AGGREGATE,
    ChangeOverTimeMetricCheck,
)
from soda.execution.check.check import Check
from soda.execution.check_outcome import CheckOutcome
from soda.execution.check_type import CheckType
from soda.execution.metric.metric import Metric
from soda.execution.partition import Partition

GROUP_BY_RESULTS = "group_by_results"


class GroupByCheck(Check):
    def __init__(
        self,
        check_cfg: GroupByCheckCfg,
        data_source_scan: DataSourceScan,
        partition: Partition,
    ):
        super().__init__(
            check_cfg=check_cfg,
            data_source_scan=data_source_scan,
            partition=partition,
            column=None,
        )

        self.check_value = None
        self.check_type = CheckType.LOCAL

        from soda.execution.metric.group_by_metric import GroupByMetric

        group_by_metric = data_source_scan.resolve_metric(
            GroupByMetric(
                data_source_scan=self.data_source_scan,
                partition=partition,
                query=self.check_cfg.query,
                check=self,
            )
        )
        self.metrics[GROUP_BY_RESULTS] = group_by_metric

    def evaluate(self, metrics: dict[str, Metric], historic_values: dict[str, object]):
        query_results = metrics[GROUP_BY_RESULTS].value
        group_limit = self.check_cfg.group_limit

        if len(query_results) > group_limit:
            raise Exception(
                f"Total number of groups {len(query_results)} exceeds configured group limit: {group_limit}"
            )

        fields = list(self.check_cfg.fields)
        # validate if configured fields are part of query_results
        all_keys = set().union(*(qr.keys() for qr in query_results))
        if not set(fields).issubset(all_keys):
            raise Exception(f"configured fields: {fields} are not found in query results.")

        group_check_cfgs = self.check_cfg.check_cfgs
        groups = [tuple(map(qr.get, fields)) for qr in query_results]

        group_checks = []

        # Group check identity is used to taint each check so that they are unique. Use last one in the dict.
        group_check_identities = self.create_identities()
        group_check_identity = group_check_identities[list(group_check_identities.keys())[-1]]

        for group in groups:
            for gcc in group_check_cfgs:
                group_name = f"{','.join(str(v) for v in group)}"
                config = copy.deepcopy(gcc)

                column = ",".join(fields)
                # TODO metrics are created in check constructors and resolved. This is unnecessary and confusing, we replace those metrics below.
                gc = Check.create(
                    check_cfg=config, data_source_scan=self.data_source_scan, partition=self.partition, column=column
                )
                if not gc.check_cfg.source_configurations:
                    gc.check_cfg.source_configurations = {}

                # Set up check name if not set by the user.
                if not gc.check_cfg.name:
                    gc.check_cfg.name = gc.check_cfg.source_line

                # Add group check identity to source_configurations so that identity of the check is unique, based on the group check, and so that check identity changes
                # when group check changes (attributes/labels).
                gc.check_cfg.source_configurations["group_check_identity"] = group_check_identity

                # Use non-labeled check identity as the group identity. Use last version of identity as this does not need to be resilient to change.
                gc_identities_no_label = gc.create_identities()
                gc_identity_no_label = gc_identities_no_label[list(gc_identities_no_label.keys())[-1]]
                gc_name_no_label = gc.name

                # Add the group label to source configuration so that identity is unique. Add the group label to the check name as well so that name of the check is unique as well.
                gc.check_cfg.source_configurations["group_value"] = f"[{group_name}]"
                gc.check_cfg.name += f" [{group_name}]"
                # Add the group label to the check custom identity if it exists as well - otherwise there would be conflicting custom identities.
                gc_custom_identity = gc.check_cfg.source_configurations.get("identity")
                if gc_custom_identity:
                    gc.check_cfg.source_configurations["identity"] += f" [{group_name}]"

                result = next(filter(lambda qr: tuple(map(qr.get, fields)) == group, query_results))
                if result is not None:
                    gc.check_value = result[config.metric_name]
                    metric = Metric(
                        self.data_source_scan,
                        self.partition,
                        column=None,
                        name=f"{config.source_line} [{group_name}]",
                        check=None,
                        identity_parts=[],
                    )

                    historic_values = {}
                    if gc.historic_descriptors:
                        # Override historic descriptors with group by ones
                        if isinstance(gc, AnomalyMetricCheck):
                            gc.historic_descriptors[KEY_HISTORIC_MEASUREMENTS] = HistoricMeasurementsDescriptor(
                                metric_identity=metric.identity,
                                limit=HISTORIC_MEASUREMENTS_LIMIT,
                            )
                            gc.historic_descriptors[KEY_HISTORIC_CHECK_RESULTS] = HistoricCheckResultsDescriptor(
                                check_identity=gc.create_identity(), limit=HISTORIC_MEASUREMENTS_LIMIT
                            )
                        elif isinstance(gc, ChangeOverTimeMetricCheck):
                            gc.historic_descriptors[KEY_HISTORIC_METRIC_AGGREGATE] = HistoricChangeOverTimeDescriptor(
                                metric_identity=metric.identity,
                                change_over_time_cfg=gc.check_cfg.change_over_time_cfg,
                            )

                        # Get the historic values.
                        for hd_key, hd in gc.historic_descriptors.items():
                            historic_values[
                                hd_key
                            ] = self.data_source_scan.scan._get_historic_data_from_soda_cloud_metric_store(hd)

                    metric.set_value(gc.check_value)
                    self.data_source_scan.scan._add_metric(metric)
                    gc.metrics = {config.metric_name: metric}
                    gc.evaluate(metrics=gc.metrics, historic_values=historic_values)

                    cloud_group_attr = {
                        "group": {
                            "identity": gc_identity_no_label,
                            "name": gc_name_no_label,
                            "distinctLabel": group_name,
                        }
                    }
                    gc.cloud_dict.update(cloud_group_attr)
                    gc.dict.update(cloud_group_attr)

                (are_attributes_valid, check_attributes) = self.validate_attributes(gc)
                if are_attributes_valid is False:
                    self.data_source_scan.scan.invalid_checks.append(gc)
                else:
                    gc.attributes = check_attributes
                    group_checks.append(gc)

        self.data_source_scan.scan._checks.extend(group_checks)

        if all(gc.outcome == CheckOutcome.PASS for gc in group_checks):
            self.outcome = CheckOutcome.PASS
        elif any(gc.outcome == CheckOutcome.FAIL for gc in group_checks):
            self.outcome = CheckOutcome.FAIL
        else:
            self.outcome = CheckOutcome.PASS

    def validate_attributes(self, gc):
        are_attributes_valid = True
        scan = self.data_source_scan.scan
        if gc.check_cfg.source_configurations:
            check_attributes = {
                scan.jinja_resolve(k): scan.jinja_resolve(v)
                for k, v in gc.check_cfg.source_configurations.get("attributes", {}).items()
            }

            if scan._configuration.soda_cloud:
                # Validate attributes if Cloud is available
                if check_attributes:
                    from soda.common.attributes_handler import AttributeHandler

                    attribute_handler = AttributeHandler(scan._logs)
                    attributes_schema = scan._configuration.soda_cloud.get_check_attributes_schema()

                    check_attributes, are_attributes_valid = attribute_handler.validate(
                        check_attributes, attributes_schema
                    )

            return (are_attributes_valid, check_attributes)

    def get_cloud_diagnostics_dict(self) -> dict:
        group_by_diagnostics = {}
        return group_by_diagnostics

    def get_log_diagnostic_lines(self) -> list[str]:
        diagnostics_texts: list[str] = []
        return diagnostics_texts
